{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "operating-novelty",
   "metadata": {},
   "source": [
    "<header>\n",
    "   <p  style='font-size:36px;font-family:Arial; color:#F0F0F0; background-color: #00233c; padding-left: 20pt; padding-top: 20pt;padding-bottom: 10pt; padding-right: 20pt;'>\n",
    "       Real-time data loading with Kafka and Teradata VantageCloud Lake\n",
    "  <br>\n",
    "       <img id=\"teradata-logo\" src=\"images/TeradataLogo.png\" alt=\"Teradata\" style=\"width: 125px; height: auto; margin-top: 20pt;\">\n",
    "    </p>\n",
    "</header>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-immigration",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<b style = 'font-size:24px;font-family:Arial;color:#00233C'>Leverage the parallelism and scale of message streaming architectures with native data loading capabilities in Teradata Vantage</b>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Streaming</b> messaging patterns are an integral component of the enterprise data landcape.  Publish/subscribe messaging systems like <b>Apache Kafka, AWS Kinesis, or Azure EventHubs</b> provide a robust, high-scale infrastructure for the receipt and delivery of near-real-time data from event-based operations.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Teradata Vantage</b> is the industry-leading Massively-Parallel-Processing analytic engine with provides the fast, scalable data processing capabilities to handle the loading and analysis of this near-real-time data.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Organizations can effectively leverage these two architectures to both simplify and scale real-time-data processing and analytics for rapid decision support, AI and ML tasks, or any other advanced analytic need that would benefit from low-latency processing. <b>Teradata Parallel Transporter</b> is a scalable and robust parallel data loading tool that can integrate with these streaming systems to provide the critical bridge between message generation and analytics.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>This demonstration will illustrate the following process</p>\n",
    "\n",
    "<table style = 'width:100%;table-layout:fixed;'>\n",
    "    <tr>\n",
    "        <td style = 'vertical-align:top' width = '30%'>\n",
    "            <ol style = 'font-size:16px;font-family:Arial'>\n",
    "                <li><b>Publish</b> messages that consist of simulated retail events to a Kafka <i>topic</i> </li>\n",
    "                <br>\n",
    "                <br>\n",
    "                <br>\n",
    "                <li>Invoke a <b>Teradata Parallel Transporter</b> job to consume messages off the stream and write them to a table in Vantage</li>\n",
    "                <br>\n",
    "                <br>\n",
    "                <br>\n",
    "                <li><b>Analyze</b> the data as it is loaded into the database</li>\n",
    "            </ol>\n",
    "        </td>\n",
    "        <td><img src = 'images/TPT_KafkaAXSMOD.png' width = '600'></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-boundary",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>Step 1 - Publish messages to the streaming system</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>For this demonstration an <b>Apache Kafka</b> cluster has been installed and configured with a message topic <i>demotopic</i> is required.</p>\n",
    "\n",
    "```bash\n",
    "$./kafka-topics.sh --create --topic demotopic --bootstrap-server <broker>:<port>\n",
    "```\n",
    "\n",
    "<ol style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "    <li>Read simulated Digital Retail Events from a local file</li>\n",
    "    <li>Connect to the Kafka Broker</li>\n",
    "    <li>Publish messages</li>\n",
    "    </ol>\n",
    "    \n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><i>This notebook uses the kafka-python library to publish messages to the Kafka infrastructure</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install the python package if necessary\n",
    "!pip install kafka-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-withdrawal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import datetime, json\n",
    "from teradataml import *\n",
    "import teradatasql\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import concurrent.futures\n",
    "from threading import current_thread, get_ident, get_native_id, Event\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# create a local dictionary of environment-specific variables\n",
    "\n",
    "# load vars json\n",
    "with open('../../vars.json', 'r') as f:\n",
    "    session_vars = json.load(f)\n",
    "\n",
    "# Use the \"data_engineer\" and Business compute group from the base setup\n",
    "host = session_vars['environment']['host']\n",
    "username = session_vars['hierarchy']['users']['business_users'][0]['username']\n",
    "password = session_vars['hierarchy']['users']['business_users'][0]['password']\n",
    "\n",
    "# Initialize the target table.\n",
    "# Truncate all records to make the row counts easier to see\n",
    "\n",
    "with teradatasql.connect(host = host, user = username, password = password) as con:\n",
    "    cur = con.cursor()\n",
    "    cur.execute('DELETE demo.kafka_retail_events;')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-stadium",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>1.1 - Read simulated digital events data</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>These records will be used as input to the Kafka stream.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-director",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_newdata = pd.read_csv('Digital_Retail_Events.csv', nrows = 1000)\n",
    "df_newdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-rehabilitation",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>1.2 - Connect to Kafka</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Use the python library to instantiate a connection as a message <i>Producer</i>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a producer object that connects to the broker(s)\n",
    "\n",
    "\n",
    "# list of strings that represent tha Kafka brokers\n",
    "bootstrap_hosts = ['<host>:<port>']\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=bootstrap_hosts)\n",
    "\n",
    "if producer.bootstrap_connected() == True:\n",
    "    print('Connected to Bootstrap server(s)')\n",
    "else:\n",
    "    print('Connection failed, check error messages')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-springfield",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>1.3 - Publish messages to the stream topic</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Spawn a separate thread to publish one message per second.  This will run in the background for the rest of the demonstration.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that will publish messages\n",
    "# use two threading event flags - one to stop processing, and one to suppress status output\n",
    "\n",
    "def produce_messages(event, print_event):\n",
    "\n",
    "    for key, val in df_newdata.iterrows():\n",
    "        \n",
    "        # check thread event - do we kill the thread\n",
    "        if event.is_set():\n",
    "            print('Thread Killed')\n",
    "            return f'Thread Killed'\n",
    "            break;\n",
    "        \n",
    "        msg = str(val['Entity_Id']) + '|' + str(datetime.datetime.now()) + '|' + val['event'] + '|' + str(val['session_id'])     \n",
    "        if print_event.is_set():\n",
    "            clear_output()\n",
    "            print(f'Message {str(key)}: {msg}', end = '\\r')\n",
    "        producer.send(topic = 'demotopic', value = msg.encode('utf-8'))\n",
    "        sleep(1)\n",
    "\n",
    "e = Event()\n",
    "p = Event()\n",
    "# create a thread pool object using concurrent.futures\n",
    "executor = concurrent.futures.ThreadPoolExecutor(max_workers = 1)\n",
    "\n",
    "# call the user function for each instance in my profile to execute them in parallel\n",
    "f = executor.submit(produce_messages, e, p)\n",
    "\n",
    "# set the print flag to print messages\n",
    "p.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-stocks",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>Step 2 - Execute the TPT job</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Teradata Parallel Transporter</b> is an object-oriented client application that provides scalable, high-speed, parallel data extraction, data loading, and data updating. These capabilities can be extended with customizations or with third-party products.  An example of a TPT script that will stream data from a Kafka topic is below.</p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>In order to execute this script, open a terminal session to a host where TPT has been installed.  Execute this using the <i>tbuild</i> command (-f is the input file, -j is the job name)</p>\n",
    "\n",
    "```bash\n",
    "$tbuild -f TPT_Kafka_Stream.txt -j jobname\n",
    "```\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>This is an example of the TPT script run in the demo:</b></p>\n",
    "\n",
    "```bash\n",
    " DEFINE JOB IMPORT_TO_TERADATA\n",
    "    DESCRIPTION 'Import data to Teradata from Kafka Server'\n",
    "\n",
    "    (\n",
    "    SET LoadTargetTable = 'demo.kafka_retail_events'\n",
    "    SET StreamTdpId         = '<db_host>'\n",
    "    SET StreamUserName      = '<db_user>'\n",
    "    SET StreamUserPassword  = '<db_password>'\n",
    "\n",
    "        STEP IMPORT_THE_DATA\n",
    "        (\n",
    "            APPLY $INSERT @LoadTargetTable TO OPERATOR ($STREAM)\n",
    "            SELECT * FROM OPERATOR ($FILE_READER()\n",
    "                ATTR\n",
    "                (\n",
    "                    PrivateLogName = 'KAFKA_TestTopic_log',\n",
    "                    AccessModuleName = 'libkafkaaxsmod.so',\n",
    "                    AccessModuleInitStr = '-MODE C\n",
    "                                           -TOPIC demotopic\n",
    "                                           -BROKERS <broker>:<port>\n",
    "                                           -BLOCKSIZE 30000\n",
    "                                           -PARTITION 0\n",
    "                                           -SHOWP y\n",
    "                                           -TRACELEVEL 3\n",
    "                                           -CONFIG compression.codec=none\n",
    "                                           -CONFIG topic.auto.offset.reset=latest\n",
    "                                           -alf Y\n",
    "                                          ',\n",
    "                     TextDelimiter = '|',\n",
    "                     Format = 'Delimited'\n",
    "\n",
    "                )\n",
    "            );\n",
    "        );\n",
    "    );\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-maintenance",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:20px;font-family:Arial;color:#00233C'><b>Step 3 - Monitor data loading from the database</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>A simple script showing the data being loaded real-time</p>\n",
    "\n",
    "<ol style = 'font-size:16px;font-family:Arial;color:#00233C'>\n",
    "    <li>Connect to the Vantage system</li>\n",
    "    <li>Query the count of records</li>\n",
    "    <li>Display an active visualization of event counts</li>\n",
    "    </ol>\n",
    "\n",
    "<hr>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>3.1 - Connect to VantageCloud Lake</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Create a client connection to the destination database.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-inclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the printing of stream publishing messages\n",
    "p.clear()\n",
    "\n",
    "\n",
    "eng = create_context(host = host, username = username, password = password)\n",
    "\n",
    "# confirm connection\n",
    "print(eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-grave",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>3.2 - Sample the data</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Query some records and watch the row count increase.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composed-macedonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a sample of the data\n",
    "DataFrame.from_query('SELECT TOP 10 * FROM demo.kafka_retail_events ORDER BY datestamp DESC;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-nirvana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the count of records increasing\n",
    "for i in range(10):\n",
    "    print(execute_sql('SELECT COUNT(*) FROM demo.kafka_retail_events;').fetchall(), end = '\\r')\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-refrigerator",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>3.3 - Visualize</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Sample histogram showing event counts.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quiet-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paint a nice plot of the count of pages increasing\n",
    "\n",
    "tdf = DataFrame('\"demo\".\"kafka_retail_events\"')\n",
    "for i in range(100):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    clear_output(wait=True)\n",
    "    sleep(3)\n",
    "    df = tdf.groupby('event').count().to_pandas(all_rows = True)\n",
    "    df.set_index('event')[['count_datestamp']].plot(kind = 'barh', ax = ax)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-convert",
   "metadata": {},
   "source": [
    "<hr>\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'><b>Cleanup</b></p>\n",
    "\n",
    "<p style = 'font-size:16px;font-family:Arial;color:#00233C'>Stop the thread from publishing messages and disconnect from the database.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill the message producer thread if desired\n",
    "e.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-hundred",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-stone",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-immune",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
